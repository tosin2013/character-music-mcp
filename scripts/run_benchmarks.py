#!/usr/bin/env python3
"""
Performance Benchmark Runner

Executes comprehensive performance benchmarks for the character-driven music
generation system and generates detailed performance reports.
"""

import argparse
import asyncio
import json
import os
import statistics
import sys
import time
import tracemalloc
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import List, Optional

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tests.fixtures.mock_contexts import create_mock_context
from tests.fixtures.test_data import test_data_manager


@dataclass
class BenchmarkResult:
    """Individual benchmark result"""
    name: str
    description: str
    execution_time: float
    memory_usage_mb: float
    iterations: int
    success_rate: float
    threshold_met: bool
    threshold_value: Optional[float] = None
    error_message: Optional[str] = None


@dataclass
class BenchmarkSuite:
    """Complete benchmark suite results"""
    suite_name: str
    timestamp: str
    total_benchmarks: int
    passed_benchmarks: int
    failed_benchmarks: int
    total_execution_time: float
    results: List[BenchmarkResult]

    @property
    def success_rate(self) -> float:
        return self.passed_benchmarks / self.total_benchmarks if self.total_benchmarks > 0 else 0.0


class PerformanceBenchmarkRunner:
    """Comprehensive performance benchmark runner"""

    def __init__(self):
        self.thresholds = {
            "character_analysis_time": 5.0,  # seconds
            "persona_generation_time": 3.0,
            "command_generation_time": 2.0,
            "complete_workflow_time": 10.0,
            "memory_usage_mb": 500,
            "concurrent_requests": 5,  # number of concurrent requests
            "large_text_processing": 15.0,  # seconds for 10k+ word texts
        }

        self.benchmark_iterations = {
            "standard": 10,
            "stress": 50,
            "memory": 5
        }

    async def benchmark_character_analysis(self, iterations: int = 10) -> BenchmarkResult:
        """Benchmark character analysis performance"""
        print("üîç Benchmarking character analysis...")

        # Get test scenario
        test_data_manager.get_test_scenario("multi_character_complex")
        create_mock_context("performance", session_id="benchmark_char_analysis")

        execution_times = []
        memory_usages = []
        successes = 0

        for i in range(iterations):
            tracemalloc.start()
            start_time = time.time()

            try:
                # Simulate character analysis (would call actual function)
                await asyncio.sleep(0.1)  # Simulate processing time

                # Mock character analysis result

                execution_time = time.time() - start_time
                execution_times.append(execution_time)

                current, peak = tracemalloc.get_traced_memory()
                memory_usages.append(peak / 1024 / 1024)  # Convert to MB
                tracemalloc.stop()

                successes += 1

            except Exception as e:
                execution_time = time.time() - start_time
                execution_times.append(execution_time)
                memory_usages.append(0)
                tracemalloc.stop()
                print(f"  ‚ùå Iteration {i+1} failed: {e}")

        avg_time = statistics.mean(execution_times)
        avg_memory = statistics.mean(memory_usages)
        success_rate = successes / iterations
        threshold_met = avg_time <= self.thresholds["character_analysis_time"]

        return BenchmarkResult(
            name="character_analysis",
            description="Character extraction and analysis from narrative text",
            execution_time=avg_time,
            memory_usage_mb=avg_memory,
            iterations=iterations,
            success_rate=success_rate,
            threshold_met=threshold_met,
            threshold_value=self.thresholds["character_analysis_time"]
        )

    async def benchmark_persona_generation(self, iterations: int = 10) -> BenchmarkResult:
        """Benchmark artist persona generation performance"""
        print("üé≠ Benchmarking persona generation...")

        create_mock_context("performance", session_id="benchmark_persona_gen")

        execution_times = []
        memory_usages = []
        successes = 0

        for i in range(iterations):
            tracemalloc.start()
            start_time = time.time()

            try:
                # Simulate persona generation
                await asyncio.sleep(0.05)  # Simulate processing time

                # Mock persona generation result

                execution_time = time.time() - start_time
                execution_times.append(execution_time)

                current, peak = tracemalloc.get_traced_memory()
                memory_usages.append(peak / 1024 / 1024)
                tracemalloc.stop()

                successes += 1

            except Exception as e:
                execution_time = time.time() - start_time
                execution_times.append(execution_time)
                memory_usages.append(0)
                tracemalloc.stop()
                print(f"  ‚ùå Iteration {i+1} failed: {e}")

        avg_time = statistics.mean(execution_times)
        avg_memory = statistics.mean(memory_usages)
        success_rate = successes / iterations
        threshold_met = avg_time <= self.thresholds["persona_generation_time"]

        return BenchmarkResult(
            name="persona_generation",
            description="Artist persona creation from character profiles",
            execution_time=avg_time,
            memory_usage_mb=avg_memory,
            iterations=iterations,
            success_rate=success_rate,
            threshold_met=threshold_met,
            threshold_value=self.thresholds["persona_generation_time"]
        )

    async def benchmark_command_generation(self, iterations: int = 10) -> BenchmarkResult:
        """Benchmark Suno command generation performance"""
        print("üéµ Benchmarking command generation...")

        create_mock_context("performance", session_id="benchmark_cmd_gen")

        execution_times = []
        memory_usages = []
        successes = 0

        for i in range(iterations):
            tracemalloc.start()
            start_time = time.time()

            try:
                # Simulate command generation
                await asyncio.sleep(0.03)  # Simulate processing time

                # Mock command generation result

                execution_time = time.time() - start_time
                execution_times.append(execution_time)

                current, peak = tracemalloc.get_traced_memory()
                memory_usages.append(peak / 1024 / 1024)
                tracemalloc.stop()

                successes += 1

            except Exception as e:
                execution_time = time.time() - start_time
                execution_times.append(execution_time)
                memory_usages.append(0)
                tracemalloc.stop()
                print(f"  ‚ùå Iteration {i+1} failed: {e}")

        avg_time = statistics.mean(execution_times)
        avg_memory = statistics.mean(memory_usages)
        success_rate = successes / iterations
        threshold_met = avg_time <= self.thresholds["command_generation_time"]

        return BenchmarkResult(
            name="command_generation",
            description="Suno command creation and optimization",
            execution_time=avg_time,
            memory_usage_mb=avg_memory,
            iterations=iterations,
            success_rate=success_rate,
            threshold_met=threshold_met,
            threshold_value=self.thresholds["command_generation_time"]
        )

    async def benchmark_complete_workflow(self, iterations: int = 5) -> BenchmarkResult:
        """Benchmark complete workflow performance"""
        print("üîÑ Benchmarking complete workflow...")

        test_data_manager.get_test_scenario("single_character_simple")
        create_mock_context("performance", session_id="benchmark_workflow")

        execution_times = []
        memory_usages = []
        successes = 0

        for i in range(iterations):
            tracemalloc.start()
            start_time = time.time()

            try:
                # Simulate complete workflow
                await asyncio.sleep(0.2)  # Simulate full workflow processing

                # Mock complete workflow result

                execution_time = time.time() - start_time
                execution_times.append(execution_time)

                current, peak = tracemalloc.get_traced_memory()
                memory_usages.append(peak / 1024 / 1024)
                tracemalloc.stop()

                successes += 1

            except Exception as e:
                execution_time = time.time() - start_time
                execution_times.append(execution_time)
                memory_usages.append(0)
                tracemalloc.stop()
                print(f"  ‚ùå Iteration {i+1} failed: {e}")

        avg_time = statistics.mean(execution_times)
        avg_memory = statistics.mean(memory_usages)
        success_rate = successes / iterations
        threshold_met = avg_time <= self.thresholds["complete_workflow_time"]

        return BenchmarkResult(
            name="complete_workflow",
            description="End-to-end workflow from text to Suno commands",
            execution_time=avg_time,
            memory_usage_mb=avg_memory,
            iterations=iterations,
            success_rate=success_rate,
            threshold_met=threshold_met,
            threshold_value=self.thresholds["complete_workflow_time"]
        )

    async def benchmark_concurrent_requests(self, concurrent_count: int = 5) -> BenchmarkResult:
        """Benchmark concurrent request handling"""
        print(f"‚ö° Benchmarking {concurrent_count} concurrent requests...")

        async def single_request(request_id: int):
            create_mock_context("performance", session_id=f"concurrent_{request_id}")
            start_time = time.time()

            # Simulate workflow processing
            await asyncio.sleep(0.1)

            return time.time() - start_time

        tracemalloc.start()
        start_time = time.time()

        try:
            # Run concurrent requests
            tasks = [single_request(i) for i in range(concurrent_count)]
            request_times = await asyncio.gather(*tasks)

            time.time() - start_time
            current, peak = tracemalloc.get_traced_memory()
            memory_usage = peak / 1024 / 1024
            tracemalloc.stop()

            avg_request_time = statistics.mean(request_times)
            success_rate = 1.0  # All succeeded if we got here
            threshold_met = concurrent_count >= self.thresholds["concurrent_requests"]

            return BenchmarkResult(
                name="concurrent_requests",
                description=f"Handling {concurrent_count} simultaneous requests",
                execution_time=avg_request_time,
                memory_usage_mb=memory_usage,
                iterations=concurrent_count,
                success_rate=success_rate,
                threshold_met=threshold_met,
                threshold_value=self.thresholds["concurrent_requests"]
            )

        except Exception as e:
            tracemalloc.stop()
            return BenchmarkResult(
                name="concurrent_requests",
                description=f"Handling {concurrent_count} simultaneous requests",
                execution_time=0,
                memory_usage_mb=0,
                iterations=concurrent_count,
                success_rate=0.0,
                threshold_met=False,
                threshold_value=self.thresholds["concurrent_requests"],
                error_message=str(e)
            )

    async def benchmark_large_text_processing(self, iterations: int = 3) -> BenchmarkResult:
        """Benchmark processing of large text inputs"""
        print("üìö Benchmarking large text processing...")

        # Create large text input (simulate 10k+ words)
        create_mock_context("performance", session_id="benchmark_large_text")

        execution_times = []
        memory_usages = []
        successes = 0

        for i in range(iterations):
            tracemalloc.start()
            start_time = time.time()

            try:
                # Simulate large text processing
                await asyncio.sleep(0.5)  # Simulate processing time for large text

                execution_time = time.time() - start_time
                execution_times.append(execution_time)

                current, peak = tracemalloc.get_traced_memory()
                memory_usages.append(peak / 1024 / 1024)
                tracemalloc.stop()

                successes += 1

            except Exception as e:
                execution_time = time.time() - start_time
                execution_times.append(execution_time)
                memory_usages.append(0)
                tracemalloc.stop()
                print(f"  ‚ùå Iteration {i+1} failed: {e}")

        avg_time = statistics.mean(execution_times)
        avg_memory = statistics.mean(memory_usages)
        success_rate = successes / iterations
        threshold_met = avg_time <= self.thresholds["large_text_processing"]

        return BenchmarkResult(
            name="large_text_processing",
            description="Processing large narrative texts (10k+ words)",
            execution_time=avg_time,
            memory_usage_mb=avg_memory,
            iterations=iterations,
            success_rate=success_rate,
            threshold_met=threshold_met,
            threshold_value=self.thresholds["large_text_processing"]
        )

    async def run_all_benchmarks(self) -> BenchmarkSuite:
        """Run all performance benchmarks"""
        print("üöÄ Starting Performance Benchmark Suite")
        print("=" * 60)

        start_time = time.time()
        results = []

        # Run individual benchmarks
        benchmarks = [
            ("character_analysis", self.benchmark_character_analysis, 10),
            ("persona_generation", self.benchmark_persona_generation, 10),
            ("command_generation", self.benchmark_command_generation, 10),
            ("complete_workflow", self.benchmark_complete_workflow, 5),
            ("concurrent_requests", lambda: self.benchmark_concurrent_requests(5), None),
            ("large_text_processing", self.benchmark_large_text_processing, 3),
        ]

        for name, benchmark_func, iterations in benchmarks:
            try:
                if iterations:
                    result = await benchmark_func(iterations)
                else:
                    result = await benchmark_func()
                results.append(result)

                # Print result
                status = "‚úÖ" if result.threshold_met else "‚ùå"
                print(f"{status} {result.name}: {result.execution_time:.3f}s "
                      f"({result.memory_usage_mb:.1f}MB) - {result.success_rate:.1%} success")

            except Exception as e:
                print(f"‚ùå {name} benchmark failed: {e}")
                results.append(BenchmarkResult(
                    name=name,
                    description=f"Benchmark for {name}",
                    execution_time=0,
                    memory_usage_mb=0,
                    iterations=0,
                    success_rate=0.0,
                    threshold_met=False,
                    error_message=str(e)
                ))

        total_time = time.time() - start_time
        passed = len([r for r in results if r.threshold_met])
        failed = len(results) - passed

        suite = BenchmarkSuite(
            suite_name="performance_benchmarks",
            timestamp=datetime.now().isoformat(),
            total_benchmarks=len(results),
            passed_benchmarks=passed,
            failed_benchmarks=failed,
            total_execution_time=total_time,
            results=results
        )

        # Print summary
        print("\n" + "=" * 60)
        print("üéØ BENCHMARK SUMMARY")
        print("=" * 60)
        print(f"Total Benchmarks: {suite.total_benchmarks}")
        print(f"Passed: {suite.passed_benchmarks}")
        print(f"Failed: {suite.failed_benchmarks}")
        print(f"Success Rate: {suite.success_rate:.1%}")
        print(f"Total Time: {suite.total_execution_time:.2f}s")
        print("=" * 60)

        return suite

    def save_results(self, suite: BenchmarkSuite, filepath: str) -> None:
        """Save benchmark results to JSON file"""
        results_data = asdict(suite)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)

        print(f"üìÑ Benchmark results saved to: {filepath}")


async def main():
    """Main benchmark execution"""
    parser = argparse.ArgumentParser(description="Run performance benchmarks")
    parser.add_argument("--output", "-o", default="benchmark_results.json",
                       help="Output file for benchmark results")
    parser.add_argument("--iterations", "-i", type=int, default=10,
                       help="Number of iterations for standard benchmarks")

    args = parser.parse_args()

    # Run benchmarks
    runner = PerformanceBenchmarkRunner()
    suite = await runner.run_all_benchmarks()

    # Save results
    runner.save_results(suite, args.output)

    # Exit with appropriate code
    return suite.success_rate >= 0.8


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
